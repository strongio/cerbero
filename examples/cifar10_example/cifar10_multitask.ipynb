{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Multi-Task\n",
    "\n",
    "In this example we're going to build the common exercise of building a CIFAR-10 classifier but with a multi-task twist: in addition to predicting the class, we're also going to estimate the average RGB values of each image.\n",
    "This will turn our classic classification example into a classification and regression multi-task learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# NOTE: for dev purposes add package to path\n",
    "cerbero_path = os.path.normpath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(cerbero_path)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset is a set of 3-channel color images of 32x32 pixels in size.\n",
    "Each image is labeled with one of the following classes:  ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’.\n",
    "\n",
    "In addition to predicting the correct label for each image we're also going to estimate the average RGB color for each image. For this we'll have to do a bit of additional preprocessing to get to the following dataset:\n",
    "\n",
    "- input: [3,32,32] float tensor of normalized range [-1,1]\n",
    "- targets:\n",
    "  - class label: int [0,9]\n",
    "  - rgb: 3-element vector of range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_cifar_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "images, class_labels, rgb_labels = make_cifar_dataset(train=True)\n",
    "\n",
    "# Further split train into train/test\n",
    "(images_train,\n",
    " images_test,\n",
    " class_labels_train,\n",
    " class_labels_test,\n",
    " rgb_labels_train,\n",
    " rgb_labels_test) = train_test_split(images, class_labels, rgb_labels, test_size=0.1)\n",
    "\n",
    "images_val, class_labels_val, rgb_labels_val = make_cifar_dataset(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now group our inputs and labels based on the tasks.\n",
    "Normally a dictionary would work just fine, but in our case we need to use the same input data but with multiple output labels.\n",
    "To avoid making a copy of our data and reducing our memory footprint it's recomended to use the `cerbero.core.Database`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cerbero.core.Database` object is a simple relation database that allows us to map multiple tasks to the same input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: `cerbero.core.Database` expects all values to be `torch.Tensor` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to torch.Tensors\n",
    "images_train = torch.Tensor(images_train)\n",
    "class_labels_train = torch.Tensor(class_labels_train).type(torch.LongTensor)\n",
    "rgb_labels_train = torch.Tensor(rgb_labels_train)\n",
    "\n",
    "images_test = torch.Tensor(images_test)\n",
    "class_labels_test = torch.Tensor(class_labels_test).type(torch.LongTensor)\n",
    "rgb_labels_test = torch.Tensor(rgb_labels_test)\n",
    "\n",
    "images_val = torch.Tensor(images_val)\n",
    "class_labels_val = torch.Tensor(class_labels_val).type(torch.LongTensor)\n",
    "rgb_labels_val = torch.Tensor(rgb_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerbero.core import Database\n",
    "\n",
    "X_train, X_val, X_test = Database(), Database(), Database()\n",
    "Y_train, Y_val, Y_test = Database(), Database(), Database()\n",
    "\n",
    "# Label class dataset\n",
    "X_train[\"class\"] = images_train\n",
    "Y_train[\"class\"] = class_labels_train\n",
    "X_test[\"class\"] = images_test\n",
    "Y_test[\"class\"] = class_labels_test\n",
    "X_val[\"class\"] = images_val\n",
    "Y_val[\"class\"] = class_labels_val\n",
    "\n",
    "# RGB dataset\n",
    "X_train[\"rgb\"] = images_train\n",
    "Y_train[\"rgb\"] = rgb_labels_train\n",
    "X_test[\"rgb\"] = images_test\n",
    "Y_test[\"rgb\"] = rgb_labels_test\n",
    "X_val[\"rgb\"] = images_val\n",
    "Y_val[\"rgb\"] = rgb_labels_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data now loaded/created, we can now package it up into `DictDataset` objects for training.\n",
    "This object is a simple wrapper around `torch.utils.data.Dataset` and stored data fields and labels as dictionaries.\n",
    "\n",
    "In the `DictDataset`, each label corresponds to a particular `Task` by name. We'll define these `Task` objects in the following section as we define our model.\n",
    "\n",
    "`DictDataLoader` is a wrapper for `torch.utils.data.DataLoader`, which handles the collate function for `DictDataset` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerbero.core import DictDataset, DictDataLoader\n",
    "\n",
    "dataloaders = []\n",
    "for task_name in [\"class\", \"rgb\"]:\n",
    "    for split, X, Y in (\n",
    "        (\"train\", X_train, Y_train),\n",
    "        (\"valid\", X_val, Y_val),\n",
    "        (\"test\", X_test, Y_test)\n",
    "    ):\n",
    "        X_dict = {f\"{task_name}_data\": torch.FloatTensor(X[task_name])}\n",
    "        YTensor = torch.FloatTensor if task_name == \"rgb\" else torch.LongTensor\n",
    "        Y_dict = {f\"{task_name}_task\": YTensor(Y[task_name])}\n",
    "        dataset = DictDataset(f\"{task_name}Dataset\", split, X_dict, Y_dict)\n",
    "        dataloader = DictDataLoader(dataset, batch_size=32)\n",
    "        dataloaders.append(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 4 data loaders, one for each split (`train`, `val`) of each task (`class_task` and `rgb_task`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the `MultitaskClassifier` model, a PyTorch multi-task classifier. We'll instantiate it from a list of `Tasks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from cerbero.core import Operation\n",
    "\n",
    "# Define the base conv net and a one-layer prediction \"head\" module\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "base_net = BaseNet()\n",
    "class_head_module = nn.Linear(84, 10)\n",
    "\n",
    "# The module pool contains all the modules this task uses\n",
    "module_pool = nn.ModuleDict(\n",
    "    {\"base_net\": base_net, \"class_head_module\": class_head_module}\n",
    ")\n",
    "\n",
    "# Op1: pull data from \"class_data\" and send through base net\n",
    "op1 = Operation(\n",
    "    name=\"base_net\",\n",
    "    module_name=\"base_net\",\n",
    "    inputs=[(\"_input_\", \"class_data\")]\n",
    ")\n",
    "\n",
    "# Op2: pass output of Op1 to the class head module\n",
    "op2 = Operation(\n",
    "    name=\"class_head\",\n",
    "    module_name=\"class_head_module\",\n",
    "    inputs=[\"base_net\"]\n",
    ")\n",
    "\n",
    "op_sequence = [op1, op2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the final operation will then go into a loss function to calculate the loss (e.g., cross-entropy) during training or an output function (e.g., softmax) to convert the logits into a prediction.\n",
    "\n",
    "Each `Task` also specifies which metrics it supports, which are bundled together in a `Scorer` object. For this tutorial, we'll just look at accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from cerbero.metrics import Scorer\n",
    "from cerbero.core import Task\n",
    "\n",
    "class_task = Task(\n",
    "    name=\"class_task\",\n",
    "    module_pool=module_pool,\n",
    "    op_sequence=op_sequence,\n",
    "    loss_func=F.cross_entropy,\n",
    "    output_func=partial(F.softmax, dim=1),\n",
    "    scorer=Scorer(metrics=[\"accuracy\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, for the RGB `Task`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the RGB `Task` differs in that we'll be training the model to estimate the average RGB colors in the image which we model here as a regression task. Additonally, we'll define the RGB head as a two-layer module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RGBHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(84, 16)\n",
    "        self.fc2 = nn.Linear(16, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "rgb_head_module = RGBHead()\n",
    "\n",
    "op_sequence = [\n",
    "    Operation(\n",
    "        name=\"base_net\",\n",
    "        module_name=\"base_net\",\n",
    "        inputs=[(\"_input_\", \"rgb_data\")]\n",
    "    ),\n",
    "    Operation(\n",
    "        name=\"rgb_head\",\n",
    "        module_name=\"rgb_head_module\",\n",
    "        inputs=[\"base_net\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "def identity_fn(x):\n",
    "    return x\n",
    "\n",
    "rgb_task = Task(\n",
    "    name=\"rgb_task\",\n",
    "    module_pool=nn.ModuleDict(\n",
    "        {\"base_net\": base_net,\n",
    "         \"rgb_head_module\": rgb_head_module}\n",
    "    ),\n",
    "    op_sequence=op_sequence,\n",
    "    loss_func=F.mse_loss,\n",
    "    output_func=identity_fn,\n",
    "    scorer=Scorer(metrics=[\"mse\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our tasks defined, constructing a model is simple: we simply pass the list of tasks in and the model constructs itself using information from the task flows.\n",
    "\n",
    "Note that the model uses the names of modules (not the modules themselves) to determine whether two modules specified by separate tasks are the same module (and should share weights) or different modules (with separate weights).\n",
    "So because both the `class_task` and `rgb_task` include \"base_net\" in their module pools, this module will be shared between the two tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerbero.models import MultitaskModel\n",
    "\n",
    "model = MultitaskModel([class_task, rgb_task])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is constructed, we can train it as we would a single-task model, using the `fit` method of a `Trainer` object. The `Trainer` supports multiple schedules or patterns for sampling from different dataloaders; the default is to randomly sample from them proportional to the number of batches, such that all data points will be seen exactly once before any are seen twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cerbero.trainer import Trainer\n",
    "\n",
    "trainer_config = {\n",
    "    \"progress_bar\": True,\n",
    "    \"n_epochs\": 15,\n",
    "    \"lr\": 2e-3,\n",
    "    \"checkpointing\": True\n",
    "}\n",
    "\n",
    "trainer = Trainer(**trainer_config)\n",
    "trainer.fit(model, dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can call the `model.score()` method to see the final performance on all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dataset.data[:16]\n",
    "rgb_labels = images.mean(axis=(1,2))\n",
    "class_labels = dataset.targets[:16]\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "torch_images, _ = next(dataiter)\n",
    "norm_rgb_labels = torch_images.mean(dim=(2,3))\n",
    "\n",
    "norm_transform = transforms.Normalize(\n",
    "    (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    ")\n",
    "norm_images = torch.stack(\n",
    "    [\n",
    "        norm_transform(torch.Tensor(_image))\n",
    "        for _image in torch_images\n",
    "    ],\n",
    "    dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    \"class_data\": norm_images,\n",
    "    \"rgb_data\": norm_images\n",
    "}\n",
    "with torch.no_grad():\n",
    "    out_dict = model(input_dict, task_names=[\"class_task\", \"rgb_task\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_out = torch.argmax(\n",
    "    F.softmax(out_dict[\"class_head\"], dim=1),\n",
    "    dim=1\n",
    ")\n",
    "rgb_out = out_dict[\"rgb_head\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"class_head\")\n",
    "print(f\"y_true: {class_labels}\")\n",
    "print(f\"y_pred: {class_out.numpy().tolist()}\")\n",
    "\n",
    "print(f\"\\nrgb_head\")\n",
    "print(f\"y_true:\\n{np.round(rgb_labels).astype(int)}\")\n",
    "print(f\"y_pred:\\n{np.round(rgb_out.numpy()*255).astype(int)}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
